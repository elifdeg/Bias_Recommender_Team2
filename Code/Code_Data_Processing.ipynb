{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d5bbbf-507b-40eb-962b-131a720f2b94",
   "metadata": {},
   "source": [
    "### Data Processing : Reproducing Popularity and Gender Bias in Music Recommenders with Cross-Domain Extension to Books\n",
    "Team 2<br>\n",
    "Elif Deger<br>\n",
    "Nataliya Kharitonova"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc28ea-b535-4955-b9bf-d253c61afdd9",
   "metadata": {},
   "source": [
    "In this notebook we have the code which we implemented for the data processeing steps before starting working on the recommender algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec22b3-6ea8-4ff1-9e89-5d6e62267e16",
   "metadata": {},
   "source": [
    "## LFM DATASET\n",
    "\n",
    "* The LFM-2b dataset used in our study is considered derivative work according to paragraph 4.1 of Last.fm’s API Terms of Service (https://www.last.fm/api/tos). The Last.fm Terms of Service further grant us a license to use this data (according to paragraph 4).\n",
    "\n",
    "* The exact dataset we are using is LFM-2b Dataset, which is a subset of LAST FM dataset and an extension of the LFM-1b dataset and was created by the respective authors of the paper we are replicating \"Analyzing Item Popularity Bias of Music Recommender Systems: Are Different Genders Equally Affected?\". Unfortunately due to licensing issues (see: https://www.cp.jku.at/datasets/LFM-2b/) the dataset is not avaliable to public. We had to contact the authors ourselves, and they were very kind to provide us with the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3588320-9ae6-4185-bd28-21fa7a5e7896",
   "metadata": {},
   "source": [
    "We start with pre-processing data for the music recommendation systems. \n",
    "In the paper the authors apply the following data pre-processing steps:\n",
    "\n",
    "\n",
    "\n",
    "1. Keep only user-track interactions with playcount (PC) greater than 1 to reduce noise.\n",
    "2. Consider only tracks listened to by at least 5 different users.\n",
    "3. Consider only users who listened to at least 5 different tracks.\n",
    "4. Restrict listening events to the last 5 years to focus on recent popularity trends.\n",
    "5. Convert interactions to binary form: 1 if user listened to the track at least once, 0 otherwise.\n",
    "6. Sample 100,000 tracks uniformly at random to include a balanced variety of track popularities.\n",
    "7. To evaluate split users into train, validation, and test sets (60-20-20) with 5-fold cross-validation, training on full interactions of train users and using 80/20 item splits for validation and testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd5b29-35b0-405e-ad9b-4dd88c8a80a5",
   "metadata": {},
   "source": [
    "#### Step 1:\n",
    "\n",
    "* Load the TSV file containing user listening counts.\n",
    "* Filter out rows where the `count` is **1 or less**.\n",
    "* Save the filtered data to a new TSV file called **`filtered-listening-counts.tsv`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e082ef2-ccef-4e5e-98e1-addce2eb7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('listening-counts.tsv', sep='\\t')\n",
    "\n",
    "# Filter rows with count > 1\n",
    "filtered_df = df[df['count'] > 1]\n",
    "num_unique_users = filtered_df['user_id'].nunique()\n",
    "num_unique_tracks = filtered_df['track_id'].nunique()\n",
    "\n",
    "print(f\"Unique user_id count after filtering: {num_unique_users}\")\n",
    "print(f\"Unique track_id count after filtering: {num_unique_tracks}\")\n",
    "\n",
    "filtered_df.to_csv('filtered-listening-counts.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b9f68-0ef8-4b7b-beb2-9bff8d9297a1",
   "metadata": {},
   "source": [
    "#### Step 2:\n",
    "\n",
    "* **Load two TSV files**:\n",
    "\n",
    "  * `filtered-listening-counts.tsv`: contains listening data filtered by count > 1.\n",
    "  * `users.tsv`: contains user metadata.\n",
    "\n",
    "* **Merge** the two datasets **on `user_id`** using an **inner join**\n",
    "\n",
    "* **Save** the merged dataset as `merged-users-listening.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fbeca4-b51c-4261-a0b2-c1b3fe730c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filtered_df = pd.read_csv('filtered-listening-counts.tsv', sep='\\t')\n",
    "users_df = pd.read_csv('users.tsv', sep='\\t')\n",
    "\n",
    "merged_df = pd.merge(users_df, filtered_df, on='user_id', how='inner')\n",
    "\n",
    "merged_df.to_csv('merged-users-listening.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(f\"Merged data saved with {merged_df['user_id'].nunique()} unique users.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c3164a-6921-4bd8-a6e0-f15f31281215",
   "metadata": {},
   "source": [
    "#### Step 2 - continue and Step 3\n",
    "\n",
    "* **Load** the merged listening and user data from `merged-users-listening.tsv`.\n",
    "\n",
    "* **Filter tracks**:\n",
    "\n",
    "  * Keeps only tracks listened to by **at least 5 unique users**.\n",
    "\n",
    "* **Filter users**:\n",
    "\n",
    "  * From the remaining data, keeps only users who have listened to **at least 5 unique tracks**.\n",
    "  \n",
    "* **Save** the final filtered dataset to `filtered-merged-listening.tsv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd493168-77e3-4a4b-86a6-d19240f65b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load merged data\n",
    "merged_df = pd.read_csv('merged-users-listening.tsv', sep='\\t')\n",
    "\n",
    "# First, filter tracks with at least 5 unique users\n",
    "track_user_counts = merged_df.groupby('track_id')['user_id'].nunique()\n",
    "tracks_to_keep = track_user_counts[track_user_counts >= 5].index\n",
    "filtered_df = merged_df[merged_df['track_id'].isin(tracks_to_keep)]\n",
    "\n",
    "# Then, filter users with at least 5 unique tracks\n",
    "user_track_counts = filtered_df.groupby('user_id')['track_id'].nunique()\n",
    "users_to_keep = user_track_counts[user_track_counts >= 5].index\n",
    "filtered_df = filtered_df[filtered_df['user_id'].isin(users_to_keep)]\n",
    "\n",
    "print(f\"Filtered data has {filtered_df['user_id'].nunique()} users and {filtered_df['track_id'].nunique()} tracks.\")\n",
    "\n",
    "filtered_df.to_csv('filtered-merged-listening.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daec850-d5ef-419b-a114-927f113036a6",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "\n",
    "* Load dataset and convert `creation_time` to datetime.\n",
    "* Keep events from 2008 onward and save.\n",
    "* Filter rows with valid demographics (`gender` m/f, valid `country`, or valid `age`).\n",
    "* Save demographic-filtered data.\n",
    "* Filter again for events since 2008 and save.\n",
    "* Exclude rows with `gender` = 'n' and `age` = -1, then save.\n",
    "* Drop rows with any missing values and save cleaned data.\n",
    "* Print counts of rows and unique users/tracks at key steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0127b5d4-1a77-4ae3-8e86-e115498651e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('filtered-merged-listening.tsv', sep='\\t')\n",
    "\n",
    "# Convert creation_time to datetime format\n",
    "df['creation_time'] = pd.to_datetime(df['creation_time'])\n",
    "\n",
    "# Define cutoff date (January 1, 2008)\n",
    "cutoff_date = pd.Timestamp('2008-01-01')\n",
    "recent_df = df[df['creation_time'] >= cutoff_date]\n",
    "\n",
    "print(f\"Remaining events after 2009: {len(recent_df)}\")\n",
    "print(f\"Unique users: {recent_df['user_id'].nunique()}\")\n",
    "print(f\"Unique tracks: {recent_df['track_id'].nunique()}\")\n",
    "\n",
    "recent_df.to_csv('filtered-since-2008-listening.tsv', sep='\\t', index=False)\n",
    "\n",
    "df = pd.read_csv('filtered-since-2008-listening.tsv', sep='\\t')\n",
    "\n",
    "# Apply demographic filtering and keep rows where at least one demographic attribute is valid\n",
    "filtered_df = df[\n",
    "    (df['gender'].isin(['m', 'f'])) |\n",
    "    (\n",
    "        df['country'].notna() &\n",
    "        (df['country'].str.strip() != '') &\n",
    "        (df['country'].str.lower() != 'nan')\n",
    "    ) |\n",
    "    (df['age'] != -1)\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Number of rows with at least one valid demographic attribute:\", len(filtered_df))\n",
    "\n",
    "num_users = filtered_df['user_id'].nunique()\n",
    "num_tracks = filtered_df['track_id'].nunique()\n",
    "print(f\"Unique user_id count: {num_users}\")\n",
    "print(f\"Unique track_id count: {num_tracks}\")\n",
    "\n",
    "filtered_df.to_csv('2.filtered-demographics-listening.tsv', sep='\\t', index=False)\n",
    "\n",
    "df = pd.read_csv('2.filtered-demographics-listening.tsv', sep='\\t')\n",
    "\n",
    "df['creation_time'] = pd.to_datetime(df['creation_time'])\n",
    "\n",
    "df_filtered = df[df['creation_time'] >= pd.Timestamp('2008-01-01')]\n",
    "\n",
    "num_users = df_filtered['user_id'].nunique()\n",
    "num_tracks = df_filtered['track_id'].nunique()\n",
    "\n",
    "print(f\"Remaining rows after 2008: {len(df_filtered)}\")\n",
    "print(f\"Unique user_id count: {num_users}\")\n",
    "print(f\"Unique track_id count: {num_tracks}\")\n",
    "\n",
    "df_filtered.to_csv('filtered-demographics-2008onward.tsv', sep='\\t', index=False)\n",
    "\n",
    "df = pd.read_csv('filtered-demographics-2008onward.tsv', sep='\\t')\n",
    "\n",
    "filtered_df = df[(df['gender'] != 'n') & (df['age'] != -1)]\n",
    "\n",
    "print(f\"Rows after excluding gender 'n' and age -1: {len(filtered_df)}\")\n",
    "print(f\"Unique user_id count: {filtered_df['user_id'].nunique()}\")\n",
    "print(f\"Unique track_id count: {filtered_df['track_id'].nunique()}\")\n",
    "\n",
    "filtered_df.to_csv('fixed-last.tsv', sep='\\t', index=False)\n",
    "\n",
    "df = pd.read_csv('fixed-last.tsv', sep='\\t')\n",
    "\n",
    "# Drop rows with any missing values\n",
    "clean_df = df.dropna()\n",
    "\n",
    "clean_df.to_csv('fixed-last-clean.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(f\"Original rows: {len(df)}\")\n",
    "print(f\"Cleaned rows (no missing values): {len(clean_df)}\")\n",
    "print(f\"Dropped rows: {len(df) - len(clean_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781ba60-0376-4202-b88f-a504fba5da78",
   "metadata": {},
   "source": [
    "#### Step 5 & Step 6\n",
    "\n",
    "* Loads a cleaned TSV dataset into `df`.\n",
    "* Create a binary column `binary_listen` set to 1 for all rows \n",
    "* Find unique tracks and samples up to 100,000 tracks randomly without replacement.\n",
    "* Filter the DataFrame to keep only the sampled tracks.\n",
    "* Drop the original `count` column to keep only binary data.\n",
    "* Save the sampled data to a new TSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e635ea-4ba6-4fd7-9660-04fa31ca496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('fixed-last-clean.tsv', sep='\\t')\n",
    "\n",
    "# Convert listening counts to binary (1 if listened at least once, 0 otherwise)\n",
    "# Since the dataset only has listening events, we just set count=1\n",
    "df['binary_listen'] = 1\n",
    "\n",
    "# Now, sample 100,000 tracks uniformly at random\n",
    "unique_tracks = df['track_id'].unique()\n",
    "\n",
    "# If less than 100k tracks, take all\n",
    "num_tracks_to_sample = min(100_000, len(unique_tracks))\n",
    "\n",
    "sampled_tracks = np.random.choice(unique_tracks, size=num_tracks_to_sample, replace=False)\n",
    "\n",
    "# Filter dataframe to keep only the sampled tracks\n",
    "final_df = df[df['track_id'].isin(sampled_tracks)].copy()\n",
    "\n",
    "final_df = final_df.drop(columns=['count'])\n",
    "\n",
    "final_df.to_csv('sampled_100k.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(f\"Sampled {num_tracks_to_sample} tracks.\")\n",
    "print(f\"Final dataset has {final_df['user_id'].nunique()} unique users and {final_df['track_id'].nunique()} unique tracks.\")\n",
    "print(f\"Total listening events (binary interactions): {len(final_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e36124-f3a1-4c8f-bb6c-01def42af95c",
   "metadata": {},
   "source": [
    "#### Step 7\n",
    "\n",
    "* Loads the dataset `'sampled_100k.tsv'` into `df`.\n",
    "* Set up 5-fold cross-validation with shuffled user splits.\n",
    "* Creates a directory `cv_splits` to store the folds.\n",
    "* For each fold:\n",
    "\n",
    "  * Split users into train+val and test sets based on KFold indices.\n",
    "  * Further split train+val users into 60% train and 20% validation users.\n",
    "  * Filter the main DataFrame into train, val, and test subsets by user IDs.\n",
    "  * Define `split_input_target` to split each user’s interactions into 80% input and 20% target by shuffling their items.\n",
    "  * Apply this split to validation and test sets (creating input and target parts).\n",
    "  * Save train, val input, val target, test input, and test target files for the current fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d0c0e-5424-44fd-9308-5a78187fea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "\n",
    "df = pd.read_csv('sampled_100k.tsv', sep='\\t')\n",
    "\n",
    "users = df['user_id'].unique()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare output dir\n",
    "os.makedirs(\"cv_splits\", exist_ok=True)\n",
    "\n",
    "# Iterate over folds\n",
    "for fold, (train_val_idx, test_idx) in enumerate(kf.split(users), start=1):\n",
    "    users_train_val = users[train_val_idx]\n",
    "    users_test = users[test_idx]\n",
    "\n",
    "    # Further split train_val into train and val (60% / 20% of total users)\n",
    "    n_total = len(users)\n",
    "    n_train = int(0.6 * n_total)\n",
    "    n_val = int(0.2 * n_total)\n",
    "\n",
    "    train_users = users_train_val[:n_train]\n",
    "    val_users = users_train_val[n_train:]\n",
    "\n",
    "    # Get user interaction subsets\n",
    "    train_df = df[df['user_id'].isin(train_users)].copy()\n",
    "    val_df = df[df['user_id'].isin(val_users)].copy()\n",
    "    test_df = df[df['user_id'].isin(users_test)].copy()\n",
    "\n",
    "    def split_input_target(user_df):\n",
    "        \"\"\"Split each user's items into 80% input, 20% target.\"\"\"\n",
    "        input_rows = []\n",
    "        target_rows = []\n",
    "        for user_id, group in user_df.groupby('user_id'):\n",
    "            items = group.sample(frac=1, random_state=42)  # Shuffle\n",
    "            split_point = int(len(items) * 0.8)\n",
    "            input_rows.append(items.iloc[:split_point])\n",
    "            target_rows.append(items.iloc[split_point:])\n",
    "        return pd.concat(input_rows), pd.concat(target_rows)\n",
    "\n",
    "    val_input, val_target = split_input_target(val_df)\n",
    "    test_input, test_target = split_input_target(test_df)\n",
    "\n",
    "    # Save all parts\n",
    "    fold_dir = f\"cv_splits/fold_{fold}\"\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "    train_df.to_csv(f\"{fold_dir}/train.tsv\", sep=\"\\t\", index=False)\n",
    "    val_input.to_csv(f\"{fold_dir}/val_input.tsv\", sep=\"\\t\", index=False)\n",
    "    val_target.to_csv(f\"{fold_dir}/val_target.tsv\", sep=\"\\t\", index=False)\n",
    "    test_input.to_csv(f\"{fold_dir}/test_input.tsv\", sep=\"\\t\", index=False)\n",
    "    test_target.to_csv(f\"{fold_dir}/test_target.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "    print(f\" Fold {fold}:\")\n",
    "    print(f\"  Train users: {len(train_users)}\")\n",
    "    print(f\"  Val users: {len(val_users)} (input: {len(val_input)}, target: {len(val_target)})\")\n",
    "    print(f\"  Test users: {len(users_test)} (input: {len(test_input)}, target: {len(test_target)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63db16-7673-433d-8762-0b6541195a5d",
   "metadata": {},
   "source": [
    "## Pre-processing of Book-Crossing Dataset\n",
    "\n",
    "For the Book-Crossing dataset we decided to keep only the books which had ratings equal or bigger than 6, since books don't have listening counts. And if an author’s books received fewer than 2 ratings across all users, those books are excluded from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955eb4e-0519-4633-ae61-37c03e3b2944",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **1. Preprocess the Data**\n",
    "\n",
    "* Filter interactions to only include ratings **≥ 6**.\n",
    "* Map gender values to standardized labels: `'female' → 'f'`, `'male' → 'm'`.\n",
    "* Add a new column `binary_listen = 1` to indicate a positive interaction (possibly for implicit feedback).\n",
    "* Remove unnecessary columns: `'Publisher'`, `'Book-Title'`, and `'Book-Author'`.\n",
    "* Convert `user_id` to string type to ensure consistency across the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Prepare for Cross-Validation**\n",
    "\n",
    "* Extract all **unique user IDs**.\n",
    "* Set up **5-fold user-based cross-validation** with shuffling and fixed random seed for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. For Each Fold (1 to 5)**\n",
    "\n",
    "#### a. **Split Users**\n",
    "\n",
    "* Split users into:\n",
    "\n",
    "  * **Train + Validation** (80% of users)\n",
    "  * **Test** (20% of users)\n",
    "* Further split the 80% into:\n",
    "\n",
    "  * **Train** (60% of total users)\n",
    "  * **Validation** (20% of total users)\n",
    "\n",
    "### 4. **Subset the DataFrame**\n",
    "\n",
    "* Create three datasets:\n",
    "\n",
    "  * `train_df`: interactions of training users.\n",
    "  * `val_df`: interactions of validation users.\n",
    "  * `test_df`: interactions of test users.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Create Input/Target Splits for Validation and Test Sets**\n",
    "\n",
    "* For each user in `val_df` and `test_df`:\n",
    "\n",
    "  * Skip users with **< 2 interactions**.\n",
    "  * Shuffle their items.\n",
    "  * Split 80% as **input** interactions, and 20% as **target**.\n",
    "* Return two DataFrames: `*_input` and `*_target` for validation and test.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Save Each Fold to Disk**\n",
    "\n",
    "* Create a new directory for each fold: `book_folds/fold_{fold}`.\n",
    "* Save the following files inside it:\n",
    "\n",
    "  * `train.tsv` — Training data.\n",
    "  * `val_input.tsv` — 80% of validation user interactions.\n",
    "  * `val_target.tsv` — Remaining 20% of validation interactions.\n",
    "  * `test_input.tsv` — 80% of test user interactions.\n",
    "  * `test_target.tsv` — Remaining 20% of test interactions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5188d63-ca12-41dd-a55c-b372ac5ca47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('book_dataset.tsv', sep='\\t')\n",
    "\n",
    "# Rename columns to match expected schema\n",
    "df = df.rename(columns={\n",
    "    'User-ID': 'user_id',\n",
    "    'ISBN': 'item_id',\n",
    "    'Book-Rating': 'rating',\n",
    "    'Author-Gender-Guess': 'gender'\n",
    "})\n",
    "\n",
    "# Filter only books with rating >= 6\n",
    "df = df[df['rating'] >= 6].copy()\n",
    "# Map gender to 'f' and 'm'\n",
    "df['gender'] = df['gender'].str.lower().map({'female': 'f', 'male': 'm'})\n",
    "# Add binary_listen column\n",
    "df['binary_listen'] = 1\n",
    "# Drop unwanted columns\n",
    "df = df.drop(columns=['Publisher', 'Book-Title', 'Book-Author'])\n",
    "\n",
    "df['user_id'] = df['user_id'].astype(str)\n",
    "\n",
    "users = df['user_id'].unique()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare output dir\n",
    "os.makedirs(\"book_folds\", exist_ok=True)\n",
    "\n",
    "# Iterate over folds\n",
    "for fold, (train_val_idx, test_idx) in enumerate(kf.split(users), start=1):\n",
    "    users_train_val = users[train_val_idx]\n",
    "    users_test = users[test_idx]\n",
    "\n",
    "    # Further split train_val into train and val (60% / 20% of total users)\n",
    "    n_total = len(users)\n",
    "    n_train = int(0.6 * n_total)\n",
    "    n_val = int(0.2 * n_total)\n",
    "\n",
    "    train_users = users_train_val[:n_train]\n",
    "    val_users = users_train_val[n_train:]\n",
    "\n",
    "    # Get user interaction subsets\n",
    "    train_df = df[df['user_id'].isin(train_users)].copy()\n",
    "    val_df = df[df['user_id'].isin(val_users)].copy()\n",
    "    test_df = df[df['user_id'].isin(users_test)].copy()\n",
    "\n",
    "    def split_input_target(user_df):\n",
    "        \"\"\"Split each user's items into 80% input, 20% target.\"\"\"\n",
    "        input_rows = []\n",
    "        target_rows = []\n",
    "        for user_id, group in user_df.groupby('user_id'):\n",
    "            if len(group) < 2:\n",
    "                continue  # Skip users with fewer than 2 interactions\n",
    "            items = group.sample(frac=1, random_state=42)  \n",
    "            split_point = int(len(items) * 0.8)\n",
    "            input_rows.append(items.iloc[:split_point])\n",
    "            target_rows.append(items.iloc[split_point:])\n",
    "        return pd.concat(input_rows), pd.concat(target_rows)\n",
    "\n",
    "    val_input, val_target = split_input_target(val_df)\n",
    "    test_input, test_target = split_input_target(test_df)\n",
    "\n",
    "    # Save \n",
    "    fold_dir = f\"book_folds/fold_{fold}\"\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "    train_df.to_csv(f\"{fold_dir}/train.tsv\", sep=\"\\t\", index=False)\n",
    "    val_input.to_csv(f\"{fold_dir}/val_input.tsv\", sep=\"\\t\", index=False)\n",
    "    val_target.to_csv(f\"{fold_dir}/val_target.tsv\", sep=\"\\t\", index=False)\n",
    "    test_input.to_csv(f\"{fold_dir}/test_input.tsv\", sep=\"\\t\", index=False)\n",
    "    test_target.to_csv(f\"{fold_dir}/test_target.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "    print(f\" Fold {fold}:\")\n",
    "    print(f\"  Train users: {len(train_users)}\")\n",
    "    print(f\"  Val users: {len(val_users)} (input: {len(val_input)}, target: {len(val_target)})\")\n",
    "    print(f\"  Test users: {len(users_test)} (input: {len(test_input)}, target: {len(test_target)})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
