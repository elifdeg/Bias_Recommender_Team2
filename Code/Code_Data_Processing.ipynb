{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d5bbbf-507b-40eb-962b-131a720f2b94",
   "metadata": {},
   "source": [
    "### Data Processing : Reproducing Popularity and Gender Bias in Music Recommenders with Cross-Domain Extension to Books\n",
    "Team 2<br>\n",
    "Elif Deger<br>\n",
    "Nataliya Kharitonova"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc28ea-b535-4955-b9bf-d253c61afdd9",
   "metadata": {},
   "source": [
    "In this notebook we have the code which we implemented for the data processeing steps before starting working on the recommender algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec22b3-6ea8-4ff1-9e89-5d6e62267e16",
   "metadata": {},
   "source": [
    "## LFM DATASET\n",
    "\n",
    "* The LFM-2b dataset used in our study is considered derivative work according to paragraph 4.1 of Last.fmâ€™s API Terms of Service (https://www.last.fm/api/tos). The Last.fm Terms of Service further grant us a license to use this data (according to paragraph 4).\n",
    "\n",
    "* The exact dataset we are using is LFM-2b Dataset, which is a subset of LAST FM dataset and an extension of the LFM-1b dataset and was created by the respective authors of the paper we are replicating \"Analyzing Item Popularity Bias of Music Recommender Systems: Are Different Genders Equally Affected?\". Unfortunately due to licensing issues (see: https://www.cp.jku.at/datasets/LFM-2b/) the dataset is not avaliable to public. We had to contact the authors ourselves, and they were very kind to provide us with the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3588320-9ae6-4185-bd28-21fa7a5e7896",
   "metadata": {},
   "source": [
    "We start with pre-processing data for the music recommendation systems. \n",
    "In the paper the authors apply the following data pre-processing steps:\n",
    "\n",
    "\n",
    "\n",
    "1. Keep only user-track interactions with playcount (PC) greater than 1 to reduce noise.\n",
    "2. Consider only tracks listened to by at least 5 different users.\n",
    "3. Consider only users who listened to at least 5 different tracks.\n",
    "4. Restrict listening events to the last 5 years to focus on recent popularity trends.\n",
    "5. Convert interactions to binary form: 1 if user listened to the track at least once, 0 otherwise.\n",
    "6. Sample 100,000 tracks uniformly at random to include a balanced variety of track popularities.\n",
    "7. To evaluate split users into train, validation, and test sets (60-20-20) with 5-fold cross-validation, training on full interactions of train users and using 80/20 item splits for validation and testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd5b29-35b0-405e-ad9b-4dd88c8a80a5",
   "metadata": {},
   "source": [
    "#### Step 1:\n",
    "\n",
    "* Load the TSV file containing user listening counts.\n",
    "* Filter out rows where the `count` is **1 or less**.\n",
    "* Save the filtered data to a new TSV file called **`filtered-listening-counts.tsv`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e082ef2-ccef-4e5e-98e1-addce2eb7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('listening-counts.tsv', sep='\\t')\n",
    "\n",
    "# Filter rows with count > 1\n",
    "filtered_df = df[df['count'] > 1]\n",
    "num_unique_users = filtered_df['user_id'].nunique()\n",
    "num_unique_tracks = filtered_df['track_id'].nunique()\n",
    "\n",
    "print(f\"Unique user_id count after filtering: {num_unique_users}\")\n",
    "print(f\"Unique track_id count after filtering: {num_unique_tracks}\")\n",
    "\n",
    "filtered_df.to_csv('filtered-listening-counts.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b9f68-0ef8-4b7b-beb2-9bff8d9297a1",
   "metadata": {},
   "source": [
    "#### Step 2:\n",
    "\n",
    "* **Load two TSV files**:\n",
    "\n",
    "  * `filtered-listening-counts.tsv`: contains listening data filtered by count > 1.\n",
    "  * `users.tsv`: contains user metadata.\n",
    "\n",
    "* **Merge** the two datasets **on `user_id`** using an **inner join**\n",
    "\n",
    "* **Save** the merged dataset as `merged-users-listening.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fbeca4-b51c-4261-a0b2-c1b3fe730c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filtered_df = pd.read_csv('filtered-listening-counts.tsv', sep='\\t')\n",
    "users_df = pd.read_csv('users.tsv', sep='\\t')\n",
    "\n",
    "merged_df = pd.merge(users_df, filtered_df, on='user_id', how='inner')\n",
    "\n",
    "merged_df.to_csv('merged-users-listening.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(f\"Merged data saved with {merged_df['user_id'].nunique()} unique users.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c3164a-6921-4bd8-a6e0-f15f31281215",
   "metadata": {},
   "source": [
    "#### Step 2 - continue and Step 3\n",
    "\n",
    "* **Load** the merged listening and user data from `merged-users-listening.tsv`.\n",
    "\n",
    "* **Filter tracks**:\n",
    "\n",
    "  * Keeps only tracks listened to by **at least 5 unique users**.\n",
    "\n",
    "* **Filter users**:\n",
    "\n",
    "  * From the remaining data, keeps only users who have listened to **at least 5 unique tracks**.\n",
    "  \n",
    "* **Save** the final filtered dataset to `filtered-merged-listening.tsv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd493168-77e3-4a4b-86a6-d19240f65b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load merged data\n",
    "merged_df = pd.read_csv('merged-users-listening.tsv', sep='\\t')\n",
    "\n",
    "# First, filter tracks with at least 5 unique users\n",
    "track_user_counts = merged_df.groupby('track_id')['user_id'].nunique()\n",
    "tracks_to_keep = track_user_counts[track_user_counts >= 5].index\n",
    "filtered_df = merged_df[merged_df['track_id'].isin(tracks_to_keep)]\n",
    "\n",
    "# Then, filter users with at least 5 unique tracks\n",
    "user_track_counts = filtered_df.groupby('user_id')['track_id'].nunique()\n",
    "users_to_keep = user_track_counts[user_track_counts >= 5].index\n",
    "filtered_df = filtered_df[filtered_df['user_id'].isin(users_to_keep)]\n",
    "\n",
    "print(f\"Filtered data has {filtered_df['user_id'].nunique()} users and {filtered_df['track_id'].nunique()} tracks.\")\n",
    "\n",
    "filtered_df.to_csv('filtered-merged-listening.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daec850-d5ef-419b-a114-927f113036a6",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "\n",
    "* Load dataset and convert `creation_time` to datetime.\n",
    "* Keep events from 2008 onward and save.\n",
    "* Filter rows with valid demographics (`gender` m/f, valid `country`, or valid `age`).\n",
    "* Save demographic-filtered data.\n",
    "* Filter again for events since 2008 and save.\n",
    "* Exclude rows with `gender` = 'n' and `age` = -1, then save.\n",
    "* Drop rows with any missing values and save cleaned data.\n",
    "* Print counts of rows and unique users/tracks at key steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0127b5d4-1a77-4ae3-8e86-e115498651e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('filtered-merged-listening.tsv', sep='\\t')\n",
    "\n",
    "# Convert creation_time to datetime format\n",
    "df['creation_time'] = pd.to_datetime(df['creation_time'])\n",
    "\n",
    "# Define cutoff date (January 1, 2008)\n",
    "cutoff_date = pd.Timestamp('2008-01-01')\n",
    "recent_df = df[df['creation_time'] >= cutoff_date]\n",
    "\n",
    "print(f\"Remaining events after 2009: {len(recent_df)}\")\n",
    "print(f\"Unique users: {recent_df['user_id'].nunique()}\")\n",
    "print(f\"Unique tracks: {recent_df['track_id'].nunique()}\")\n",
    "\n",
    "recent_df.to_csv('filtered-since-2008-listening.tsv', sep='\\t', index=False)\n",
    "\n",
    "df = pd.read_csv('filtered-since-2008-listening.tsv', sep='\\t')\n",
    "\n",
    "# Apply demographic filtering and keep rows where at least one demographic attribute is valid\n",
    "filtered_df = df[\n",
    "    (df['gender'].isin(['m', 'f'])) |\n",
    "    (\n",
    "        df['country'].notna() &\n",
    "        (df['country'].str.strip() != '') &\n",
    "        (df['country'].str.lower() != 'nan')\n",
    "    ) |\n",
    "    (df['age'] != -1)\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Number of rows with at least one valid demographic attribute:\", len(filtered_df))\n",
    "\n",
    "num_users = filtered_df['user_id'].nunique()\n",
    "num_tracks = filtered_df['track_id'].nunique()\n",
    "print(f\"Unique user_id count: {num_users}\")\n",
    "print(f\"Unique track_id count: {num_tracks}\")\n",
    "\n",
    "filtered_df.to_csv('2.filtered-demographics-listening.tsv', sep='\\t', index=False)\n",
    "\n",
    "df = pd.read_csv('2.filtered-demographics-listening.tsv', sep='\\t')\n",
    "\n",
    "df['creation_time'] = pd.to_datetime(df['creation_time'])\n",
    "\n",
    "df_filtered = df[df['creation_time'] >= pd.Timestamp('2008-01-01')]\n",
    "\n",
    "num_users = df_filtered['user_id'].nunique()\n",
    "num_tracks = df_filtered['track_id'].nunique()\n",
    "\n",
    "print(f\"Remaining rows after 2008: {len(df_filtered)}\")\n",
    "print(f\"Unique user_id count: {num_users}\")\n",
    "print(f\"Unique track_id count: {num_tracks}\")\n",
    "\n",
    "df_filtered.to_csv('filtered-demographics-2008onward.tsv', sep='\\t', index=False)\n",
    "\n",
    "df = pd.read_csv('filtered-demographics-2008onward.tsv', sep='\\t')\n",
    "\n",
    "filtered_df = df[(df['gender'] != 'n') & (df['age'] != -1)]\n",
    "\n",
    "print(f\"Rows after excluding gender 'n' and age -1: {len(filtered_df)}\")\n",
    "print(f\"Unique user_id count: {filtered_df['user_id'].nunique()}\")\n",
    "print(f\"Unique track_id count: {filtered_df['track_id'].nunique()}\")\n",
    "\n",
    "filtered_df.to_csv('fixed-last.tsv', sep='\\t', index=False)\n",
    "\n",
    "df = pd.read_csv('fixed-last.tsv', sep='\\t')\n",
    "\n",
    "# Drop rows with any missing values\n",
    "clean_df = df.dropna()\n",
    "\n",
    "clean_df.to_csv('fixed-last-clean.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(f\"Original rows: {len(df)}\")\n",
    "print(f\"Cleaned rows (no missing values): {len(clean_df)}\")\n",
    "print(f\"Dropped rows: {len(df) - len(clean_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781ba60-0376-4202-b88f-a504fba5da78",
   "metadata": {},
   "source": [
    "#### Step 5 & Step 6\n",
    "\n",
    "* Loads a cleaned TSV dataset into `df`.\n",
    "* Create a binary column `binary_listen` set to 1 for all rows \n",
    "* Find unique tracks and samples up to 100,000 tracks randomly without replacement.\n",
    "* Filter the DataFrame to keep only the sampled tracks.\n",
    "* Drop the original `count` column to keep only binary data.\n",
    "* Save the sampled data to a new TSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e635ea-4ba6-4fd7-9660-04fa31ca496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('fixed-last-clean.tsv', sep='\\t')\n",
    "\n",
    "# Convert listening counts to binary (1 if listened at least once, 0 otherwise)\n",
    "# Since the dataset only has listening events, we just set count=1\n",
    "df['binary_listen'] = 1\n",
    "\n",
    "# Now, sample 100,000 tracks uniformly at random\n",
    "unique_tracks = df['track_id'].unique()\n",
    "\n",
    "# If less than 100k tracks, take all\n",
    "num_tracks_to_sample = min(100_000, len(unique_tracks))\n",
    "\n",
    "sampled_tracks = np.random.choice(unique_tracks, size=num_tracks_to_sample, replace=False)\n",
    "\n",
    "# Filter dataframe to keep only the sampled tracks\n",
    "final_df = df[df['track_id'].isin(sampled_tracks)].copy()\n",
    "\n",
    "final_df = final_df.drop(columns=['count'])\n",
    "\n",
    "final_df.to_csv('sampled_100k.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(f\"Sampled {num_tracks_to_sample} tracks.\")\n",
    "print(f\"Final dataset has {final_df['user_id'].nunique()} unique users and {final_df['track_id'].nunique()} unique tracks.\")\n",
    "print(f\"Total listening events (binary interactions): {len(final_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e36124-f3a1-4c8f-bb6c-01def42af95c",
   "metadata": {},
   "source": [
    "#### Step 7\n",
    "\n",
    "* Loads the dataset `'sampled_100k.tsv'` into `df`.\n",
    "* Set up 5-fold cross-validation with shuffled user splits.\n",
    "* Creates a directory `cv_splits` to store the folds.\n",
    "* For each fold:\n",
    "\n",
    "  * Split users into train+val and test sets based on KFold indices.\n",
    "  * Further split train+val users into 60% train and 20% validation users.\n",
    "  * Filter the main DataFrame into train, val, and test subsets by user IDs.\n",
    "  * Define `split_input_target` to split each userâ€™s interactions into 80% input and 20% target by shuffling their items.\n",
    "  * Apply this split to validation and test sets (creating input and target parts).\n",
    "  * Save train, val input, val target, test input, and test target files for the current fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d0c0e-5424-44fd-9308-5a78187fea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "\n",
    "df = pd.read_csv('sampled_100k.tsv', sep='\\t')\n",
    "\n",
    "users = df['user_id'].unique()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare output dir\n",
    "os.makedirs(\"cv_splits\", exist_ok=True)\n",
    "\n",
    "# Iterate over folds\n",
    "for fold, (train_val_idx, test_idx) in enumerate(kf.split(users), start=1):\n",
    "    users_train_val = users[train_val_idx]\n",
    "    users_test = users[test_idx]\n",
    "\n",
    "    # Further split train_val into train and val (60% / 20% of total users)\n",
    "    n_total = len(users)\n",
    "    n_train = int(0.6 * n_total)\n",
    "    n_val = int(0.2 * n_total)\n",
    "\n",
    "    train_users = users_train_val[:n_train]\n",
    "    val_users = users_train_val[n_train:]\n",
    "\n",
    "    # Get user interaction subsets\n",
    "    train_df = df[df['user_id'].isin(train_users)].copy()\n",
    "    val_df = df[df['user_id'].isin(val_users)].copy()\n",
    "    test_df = df[df['user_id'].isin(users_test)].copy()\n",
    "\n",
    "    def split_input_target(user_df):\n",
    "        \"\"\"Split each user's items into 80% input, 20% target.\"\"\"\n",
    "        input_rows = []\n",
    "        target_rows = []\n",
    "        for user_id, group in user_df.groupby('user_id'):\n",
    "            items = group.sample(frac=1, random_state=42)  # Shuffle\n",
    "            split_point = int(len(items) * 0.8)\n",
    "            input_rows.append(items.iloc[:split_point])\n",
    "            target_rows.append(items.iloc[split_point:])\n",
    "        return pd.concat(input_rows), pd.concat(target_rows)\n",
    "\n",
    "    val_input, val_target = split_input_target(val_df)\n",
    "    test_input, test_target = split_input_target(test_df)\n",
    "\n",
    "    # Save all parts\n",
    "    fold_dir = f\"cv_splits/fold_{fold}\"\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "    train_df.to_csv(f\"{fold_dir}/train.tsv\", sep=\"\\t\", index=False)\n",
    "    val_input.to_csv(f\"{fold_dir}/val_input.tsv\", sep=\"\\t\", index=False)\n",
    "    val_target.to_csv(f\"{fold_dir}/val_target.tsv\", sep=\"\\t\", index=False)\n",
    "    test_input.to_csv(f\"{fold_dir}/test_input.tsv\", sep=\"\\t\", index=False)\n",
    "    test_target.to_csv(f\"{fold_dir}/test_target.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "    print(f\" Fold {fold}:\")\n",
    "    print(f\"  Train users: {len(train_users)}\")\n",
    "    print(f\"  Val users: {len(val_users)} (input: {len(val_input)}, target: {len(val_target)})\")\n",
    "    print(f\"  Test users: {len(users_test)} (input: {len(test_input)}, target: {len(test_target)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63db16-7673-433d-8762-0b6541195a5d",
   "metadata": {},
   "source": [
    "## Pre-processing of Book-Crossing Dataset\n",
    "\n",
    "For the Book-Crossing dataset we decided to keep only the books which had ratings equal or bigger than 6, since books don't have listening counts. And if an authorâ€™s books received fewer than 2 ratings across all users, those books are excluded from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955eb4e-0519-4633-ae61-37c03e3b2944",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **1. Preprocess the Data**\n",
    "\n",
    "* Filter interactions to only include ratings **â‰¥ 6**.\n",
    "* Map gender values to standardized labels: `'female' â†’ 'f'`, `'male' â†’ 'm'`.\n",
    "* Add a new column `binary_listen = 1` to indicate a positive interaction (possibly for implicit feedback).\n",
    "* Remove unnecessary columns: `'Publisher'`, `'Book-Title'`, and `'Book-Author'`.\n",
    "* Convert `user_id` to string type to ensure consistency across the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Prepare for Cross-Validation**\n",
    "\n",
    "* Extract all **unique user IDs**.\n",
    "* Set up **5-fold user-based cross-validation** with shuffling and fixed random seed for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. For Each Fold (1 to 5)**\n",
    "\n",
    "#### a. **Split Users**\n",
    "\n",
    "* Split users into:\n",
    "\n",
    "  * **Train + Validation** (80% of users)\n",
    "  * **Test** (20% of users)\n",
    "* Further split the 80% into:\n",
    "\n",
    "  * **Train** (60% of total users)\n",
    "  * **Validation** (20% of total users)\n",
    "\n",
    "### 4. **Subset the DataFrame**\n",
    "\n",
    "* Create three datasets:\n",
    "\n",
    "  * `train_df`: interactions of training users.\n",
    "  * `val_df`: interactions of validation users.\n",
    "  * `test_df`: interactions of test users.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Create Input/Target Splits for Validation and Test Sets**\n",
    "\n",
    "* For each user in `val_df` and `test_df`:\n",
    "\n",
    "  * Skip users with **< 2 interactions**.\n",
    "  * Shuffle their items.\n",
    "  * Split 80% as **input** interactions, and 20% as **target**.\n",
    "* Return two DataFrames: `*_input` and `*_target` for validation and test.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Save Each Fold to Disk**\n",
    "\n",
    "* Create a new directory for each fold: `book_folds/fold_{fold}`.\n",
    "* Save the following files inside it:\n",
    "\n",
    "  * `train.tsv` â€” Training data.\n",
    "  * `val_input.tsv` â€” 80% of validation user interactions.\n",
    "  * `val_target.tsv` â€” Remaining 20% of validation interactions.\n",
    "  * `test_input.tsv` â€” 80% of test user interactions.\n",
    "  * `test_target.tsv` â€” Remaining 20% of test interactions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5188d63-ca12-41dd-a55c-b372ac5ca47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('book_dataset.tsv', sep='\\t')\n",
    "\n",
    "# Rename columns to match expected schema\n",
    "df = df.rename(columns={\n",
    "    'User-ID': 'user_id',\n",
    "    'ISBN': 'item_id',\n",
    "    'Book-Rating': 'rating',\n",
    "    'Author-Gender-Guess': 'gender'\n",
    "})\n",
    "\n",
    "# Filter only books with rating >= 6\n",
    "df = df[df['rating'] >= 6].copy()\n",
    "# Map gender to 'f' and 'm'\n",
    "df['gender'] = df['gender'].str.lower().map({'female': 'f', 'male': 'm'})\n",
    "# Add binary_listen column\n",
    "df['binary_listen'] = 1\n",
    "# Drop unwanted columns\n",
    "df = df.drop(columns=['Publisher', 'Book-Title', 'Book-Author'])\n",
    "\n",
    "df['user_id'] = df['user_id'].astype(str)\n",
    "\n",
    "users = df['user_id'].unique()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare output dir\n",
    "os.makedirs(\"book_folds\", exist_ok=True)\n",
    "\n",
    "# Iterate over folds\n",
    "for fold, (train_val_idx, test_idx) in enumerate(kf.split(users), start=1):\n",
    "    users_train_val = users[train_val_idx]\n",
    "    users_test = users[test_idx]\n",
    "\n",
    "    # Further split train_val into train and val (60% / 20% of total users)\n",
    "    n_total = len(users)\n",
    "    n_train = int(0.6 * n_total)\n",
    "    n_val = int(0.2 * n_total)\n",
    "\n",
    "    train_users = users_train_val[:n_train]\n",
    "    val_users = users_train_val[n_train:]\n",
    "\n",
    "    # Get user interaction subsets\n",
    "    train_df = df[df['user_id'].isin(train_users)].copy()\n",
    "    val_df = df[df['user_id'].isin(val_users)].copy()\n",
    "    test_df = df[df['user_id'].isin(users_test)].copy()\n",
    "\n",
    "    def split_input_target(user_df):\n",
    "        \"\"\"Split each user's items into 80% input, 20% target.\"\"\"\n",
    "        input_rows = []\n",
    "        target_rows = []\n",
    "        for user_id, group in user_df.groupby('user_id'):\n",
    "            if len(group) < 2:\n",
    "                continue  # Skip users with fewer than 2 interactions\n",
    "            items = group.sample(frac=1, random_state=42)  \n",
    "            split_point = int(len(items) * 0.8)\n",
    "            input_rows.append(items.iloc[:split_point])\n",
    "            target_rows.append(items.iloc[split_point:])\n",
    "        return pd.concat(input_rows), pd.concat(target_rows)\n",
    "\n",
    "    val_input, val_target = split_input_target(val_df)\n",
    "    test_input, test_target = split_input_target(test_df)\n",
    "\n",
    "    # Save \n",
    "    fold_dir = f\"book_folds/fold_{fold}\"\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "    train_df.to_csv(f\"{fold_dir}/train.tsv\", sep=\"\\t\", index=False)\n",
    "    val_input.to_csv(f\"{fold_dir}/val_input.tsv\", sep=\"\\t\", index=False)\n",
    "    val_target.to_csv(f\"{fold_dir}/val_target.tsv\", sep=\"\\t\", index=False)\n",
    "    test_input.to_csv(f\"{fold_dir}/test_input.tsv\", sep=\"\\t\", index=False)\n",
    "    test_target.to_csv(f\"{fold_dir}/test_target.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "    print(f\" Fold {fold}:\")\n",
    "    print(f\"  Train users: {len(train_users)}\")\n",
    "    print(f\"  Val users: {len(val_users)} (input: {len(val_input)}, target: {len(val_target)})\")\n",
    "    print(f\"  Test users: {len(users_test)} (input: {len(test_input)}, target: {len(test_target)})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
